{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtuE9LayDKXk"
      },
      "source": [
        "## Importing the relevant packagess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8i5zgukN25zJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,Dropout,BatchNormalization,GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import scipy.ndimage\n",
        "from skimage.transform import resize\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnTflF9nDY75"
      },
      "source": [
        "## Uploading and Preparing the data for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI8lE7uw28P0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkLnD3uDQ2IF"
      },
      "outputs": [],
      "source": [
        "benign_dir = os.path.join('/content/drive/My Drive/BreaKHis_v1/BreaKHis_v1/histology_slides/breast/benign')\n",
        "malignant_dir = os.path.join('/content/drive/My Drive/BreaKHis_v1/BreaKHis_v1/histology_slides/breast/malignant')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA9jjfweqMAi"
      },
      "outputs": [],
      "source": [
        "# Define label mapping (0 for benign, 1 for malignant)\n",
        "benign_label = 0\n",
        "malignant_label = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzPGhMY_da7U"
      },
      "outputs": [],
      "source": [
        "def collect_files_with_labels(directory, label):\n",
        "    file_paths = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.png'):  # Only look for .png files\n",
        "                file_paths.append((os.path.join(root, file), label))  # Store file path and its label\n",
        "    return file_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ENRSc1xdd2T"
      },
      "outputs": [],
      "source": [
        "# Collect benign and malignant files with respective labels\n",
        "benign_files = collect_files_with_labels(benign_dir, benign_label)\n",
        "malignant_files = collect_files_with_labels(malignant_dir, malignant_label)\n",
        "# Count the number of benign and malignant files\n",
        "num_benign_files = len(benign_files)  # Count of benign images\n",
        "num_malignant_files = len(malignant_files)  # Count of malignant images\n",
        "\n",
        "# Print the counts\n",
        "print(f\"Number of benign files: {num_benign_files}\")\n",
        "print(f\"Number of malignant files: {num_malignant_files}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP8z7DsSrIvN"
      },
      "outputs": [],
      "source": [
        "def convert_images_to_structured_arrays(benign_file_paths, malignant_file_paths):\n",
        "    # Lists to hold image data and labels\n",
        "    benign_images = []\n",
        "    malignant_images = []\n",
        "\n",
        "    # Process benign images\n",
        "    for file_path, label in benign_file_paths:\n",
        "        try:\n",
        "            img = Image.open(file_path).convert('RGB')\n",
        "            img = img.resize((224, 224))\n",
        "            img_array = np.array(img)\n",
        "            benign_images.append(img_array)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading benign image {file_path}: {e}\")\n",
        "\n",
        "    # Process malignant images\n",
        "    for file_path, label in malignant_file_paths:\n",
        "        try:\n",
        "            img = Image.open(file_path).convert('RGB')\n",
        "            img = img.resize((224, 224))\n",
        "            img_array = np.array(img)\n",
        "            malignant_images.append(img_array)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading malignant image {file_path}: {e}\")\n",
        "\n",
        "    # Create structured arrays for benign and malignant\n",
        "    benign_structured_array = np.empty(len(benign_images), dtype=[('image', np.uint8, (224, 224, 3)), ('label', np.int32)])\n",
        "    malignant_structured_array = np.empty(len(malignant_images), dtype=[('image', np.uint8, (224, 224, 3)), ('label', np.int32)])\n",
        "\n",
        "    # Populate structured arrays\n",
        "    for i in range(len(benign_images)):\n",
        "        benign_structured_array[i] = (benign_images[i], benign_label)\n",
        "\n",
        "    for i in range(len(malignant_images)):\n",
        "        malignant_structured_array[i] = (malignant_images[i], malignant_label)\n",
        "\n",
        "    # Combine both structured arrays into one\n",
        "    combined_structured_array = np.concatenate((benign_structured_array, malignant_structured_array))\n",
        "\n",
        "    return combined_structured_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Snwr-_OrGML"
      },
      "outputs": [],
      "source": [
        "# Convert images to a single structured array\n",
        "combined_structured_array = convert_images_to_structured_arrays(benign_files, malignant_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMjXlgabrGar"
      },
      "outputs": [],
      "source": [
        "# Output some statistics\n",
        "print(f\"Total images in combined array: {len(combined_structured_array)}\")\n",
        "print(f\"First image shape: {combined_structured_array[0]['image'].shape}\")\n",
        "print(f\"First image label: {combined_structured_array[0]['label']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-kmzsmaujgO"
      },
      "outputs": [],
      "source": [
        "# Split the combined structured array into images and labels\n",
        "images = combined_structured_array['image']  # Extract all images\n",
        "labels = combined_structured_array['label']  # Extract all labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtJRaDg_DwBS"
      },
      "source": [
        "## Splitting the data into Train, Validation and Test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YRs_sohD-sh"
      },
      "outputs": [],
      "source": [
        "# Separate data for a hold-out test set\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(images, labels, test_size=0.10, random_state=42)\n",
        "print(f'Temp set shape: {X_temp.shape}, Test set shape: {X_test.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8qxjaJ9rGfr"
      },
      "outputs": [],
      "source": [
        "# Split the data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=2/9, random_state=42)\n",
        "print(f'Training set shape: {X_train.shape}, Validation set shape: {X_val.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ],
      "metadata": {
        "id": "Mcjl06d60e60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)"
      ],
      "metadata": {
        "id": "_eKlQEcB0fRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboWO-z-23bt"
      },
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHXBLuaNLjiO"
      },
      "outputs": [],
      "source": [
        "# Define number of classes\n",
        "num_classes = 2  # 0 for benign, 1 for malignant\n",
        "\n",
        "model_1 = Sequential()\n",
        "\n",
        "# First Convolutional Layer\n",
        "model_1.add(Conv2D(16, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)))\n",
        "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second Convolutional Layer\n",
        "model_1.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))  # Increased filters\n",
        "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "\n",
        "# Flatten Layer\n",
        "model_1.add(Flatten())\n",
        "\n",
        "# Fully Connected Layer\n",
        "model_1.add(Dense(128, activation='relu'))\n",
        "model_1.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "# Output Layer\n",
        "model_1.add(Dense(1, activation='sigmoid'))  # Change to a single neuron with sigmoid activation\n",
        "\n",
        "# Build the model\n",
        "model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary to see the architecture\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlIt3tMPLppn"
      },
      "outputs": [],
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=4, restore_best_weights=True)\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=2)\n",
        "\n",
        "history_1 = model_1.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=8),  # Augmented batches\n",
        "    steps_per_epoch=len(X_train) // 8,\n",
        "    epochs=10,\n",
        "    validation_data=(X_val, y_val), callbacks = [early_stopping, lr_scheduler]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUIgRbF1LqYY"
      },
      "outputs": [],
      "source": [
        "acc_1 = history_1.history['accuracy']\n",
        "val_acc_1 = history_1.history['val_accuracy']\n",
        "print(\"accuracy = \",acc_1)\n",
        "print(\"val_accuracy = \",val_acc_1)\n",
        "loss_1 = history_1.history['loss']\n",
        "val_loss_1 = history_1.history['val_loss']\n",
        "print(\"loss = \",loss_1)\n",
        "print(\"val_loss = \",val_loss_1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LnkwCJWCBN4"
      },
      "outputs": [],
      "source": [
        "epochs_range_1 = range(len(acc_1))\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range_1, acc_1, label='Training Accuracy')\n",
        "plt.plot(epochs_range_1, val_acc_1, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range_1, loss_1, label='Training Loss')\n",
        "plt.plot(epochs_range_1, val_loss_1, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtLeCx_43IOZ"
      },
      "source": [
        "## InceptionNetV3 Based Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcYVdR59-2Oa"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import InceptionV3\n",
        "\n",
        "# Load InceptionV3 with pre-trained ImageNet weights, excluding the top layer\n",
        "base_model_Inception = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers to retain pre-trained weights\n",
        "base_model_Inception.trainable = False\n",
        "\n",
        "# Add custom layers\n",
        "x_2 = base_model_Inception.output\n",
        "x_2 = GlobalAveragePooling2D()(x_2)  # Global Average Pooling\n",
        "x_2 = Dense(128, activation='relu')(x_2)  # Fully connected layer\n",
        "x_2 = Dropout(0.5)(x_2)  # Dropout for regularization\n",
        "output = Dense(1, activation='sigmoid')(x_2)  # Binary classification output\n",
        "\n",
        "# Create the final model\n",
        "model_2 = Model(inputs=base_model_Inception.input, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model_2.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_2.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC7Yu-KP4ubA"
      },
      "outputs": [],
      "source": [
        "# Training with early stopping and learning rate reduction\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3)\n",
        "\n",
        "history_2 = model_2.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=8),  # Augmented batches\n",
        "    steps_per_epoch=len(X_train) // 8,\n",
        "    epochs=10,\n",
        "    validation_data=(X_val, y_val), callbacks = [early_stopping, lr_scheduler]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEJ46V8t4ubA"
      },
      "outputs": [],
      "source": [
        "acc_2 = history_2.history['accuracy']\n",
        "val_acc_2 = history_2.history['val_accuracy']\n",
        "print(\"accuracy = \",acc_2)\n",
        "print(\"val_accuracy = \",val_acc_2)\n",
        "loss_2 = history_2.history['loss']\n",
        "val_loss_2 = history_2.history['val_loss']\n",
        "print(\"loss = \",loss_2)\n",
        "print(\"val_loss = \",val_loss_2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2K_agui4ubA"
      },
      "outputs": [],
      "source": [
        "epochs_range_2 = range(len(acc_2))\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range_2, acc_2, label='Training Accuracy')\n",
        "plt.plot(epochs_range_2, val_acc_2, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range_2, loss_2, label='Training Loss')\n",
        "plt.plot(epochs_range_2, val_loss_2, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ye8v9Yq3kQ7"
      },
      "source": [
        "## ResNet50 Based Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjLzqFHS5AWQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load ResNet50 with pre-trained ImageNet weights, excluding the top layer\n",
        "base_model_ResNet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers to retain pre-trained weights\n",
        "base_model_ResNet.trainable = False\n",
        "\n",
        "# Add custom layers\n",
        "x_3 = base_model_ResNet.output\n",
        "x_3 = GlobalAveragePooling2D()(x_3)  # Global Average Pooling\n",
        "x_3 = Dense(128, activation='relu')(x_3)  # Fully connected layer with 128 units\n",
        "x_3 = Dropout(0.5)(x_3)  # Regularization with dropout\n",
        "output_3 = Dense(1, activation='sigmoid')(x_3)  # Output layer for binary classification\n",
        "\n",
        "# Create the final model\n",
        "model_3 = Model(inputs=base_model_ResNet.input, outputs=output_3)\n",
        "\n",
        "# Compile the model\n",
        "model_3.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_3.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avLCJn8N5AWR"
      },
      "outputs": [],
      "source": [
        "# Training with early stopping and learning rate reduction\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3)\n",
        "\n",
        "history_3 = model_3.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=8),  # Augmented batches\n",
        "    steps_per_epoch=len(X_train) // 8,\n",
        "    epochs=10,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFBj7keV5AWR"
      },
      "outputs": [],
      "source": [
        "acc_3 = history_3.history['accuracy']\n",
        "val_acc_3 = history_3.history['val_accuracy']\n",
        "print(\"accuracy = \",acc_3)\n",
        "print(\"val_accuracy = \",val_acc_3)\n",
        "loss_3 = history_3.history['loss']\n",
        "val_loss_3 = history_3.history['val_loss']\n",
        "print(\"loss = \",loss_3)\n",
        "print(\"val_loss = \",val_loss_3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz_CwiEz5AWR"
      },
      "outputs": [],
      "source": [
        "epochs_range_3 = range(len(acc_3))\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range_3, acc_3, label='Training Accuracy')\n",
        "plt.plot(epochs_range_3, val_acc_3, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range_3, loss_3, label='Training Loss')\n",
        "plt.plot(epochs_range_3, val_loss_3, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRVzERSF3vRr"
      },
      "source": [
        "## EfficientNetB0 Based Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNvZtNrY5LY-"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "\n",
        "# Build EfficientNetB0 model\n",
        "base_model_EfficientNet = EfficientNetB0(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(224, 224, 3))\n",
        "base_model_EfficientNet.trainable = False\n",
        "\n",
        "# Add custom layers\n",
        "x_4 = base_model_EfficientNet.output\n",
        "x_4 = GlobalAveragePooling2D()(x_4)  # Global Average Pooling\n",
        "x_4 = Dense(128, activation='relu')(x_4)  # Fully connected layer with 128 units\n",
        "x_4 = Dropout(0.5)(x_4)  # Regularization with dropout\n",
        "output_4 = Dense(1, activation='sigmoid')(x_4)  # Output layer for binary classification\n",
        "\n",
        "# Create the final model\n",
        "model_4 = Model(inputs=base_model_EfficientNet.input, outputs=output_4)\n",
        "\n",
        "# Compile the model\n",
        "model_4.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_4.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Tekymta5LY-"
      },
      "outputs": [],
      "source": [
        "# Training with early stopping and learning rate reduction\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=3)\n",
        "\n",
        "history_4 = model_4.fit(\n",
        "    datagen.flow(X_train, y_train, batch_size=8),  # Augmented batches\n",
        "    steps_per_epoch=len(X_train) // 8,\n",
        "    epochs=10,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YypZcyPW5LY-"
      },
      "outputs": [],
      "source": [
        "acc_4 = history_4.history['accuracy']\n",
        "val_acc_4 = history_4.history['val_accuracy']\n",
        "print(\"accuracy = \",acc_4)\n",
        "print(\"val_accuracy = \",val_acc_4)\n",
        "loss_4 = history_4.history['loss']\n",
        "val_loss_4 = history_4.history['val_loss']\n",
        "print(\"loss = \",loss_4)\n",
        "print(\"val_loss = \",val_loss_4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8TrXDHE5LY_"
      },
      "outputs": [],
      "source": [
        "epochs_range_4 = range(len(acc_4))\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range_4, acc_4, label='Training Accuracy')\n",
        "plt.plot(epochs_range_4, val_acc_4, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range_4, loss_4, label='Training Loss')\n",
        "plt.plot(epochs_range_4, val_loss_4, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EOrdNWZ5pep"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRLJvYqckjUE"
      },
      "source": [
        "Compare all 4 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rWXrpHPKAx9"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for i, model in enumerate([model_1, model_2, model_3, model_4], start=1):\n",
        "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Model {i}: Test Loss = {loss:.4f}, Test Accuracy = {accuracy:.4f}\")\n",
        "    results.append((f\"Model {i}\", loss, accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49HsdLzvKlIE"
      },
      "source": [
        "A table for better comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB-RlTGSKA-T"
      },
      "outputs": [],
      "source": [
        "# Print\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(\"{:<10} {:<10} {:<10}\".format(\"Model\", \"Loss\", \"Accuracy\"))\n",
        "for name, loss, acc in results:\n",
        "    print(f\"{name:<10} {loss:<10.4f} {acc:<10.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8SVT3mtvLv7"
      },
      "source": [
        "Confusion Matrices and other scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
        "    # Predict probabilities\n",
        "    y_pred_proba = model.predict(X_test)\n",
        "\n",
        "    # Convert probabilities to class labels\n",
        "    y_pred = (y_pred_proba > 0.5).astype(\"int32\").flatten()\n",
        "    y_true = y_test.flatten()\n",
        "\n",
        "    # Confusion Matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"Class 0\", \"Class 1\"])\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(5, 4))\n",
        "    disp.plot(cmap=\"Blues\", values_format='d')\n",
        "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # F1 Score\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    print(f\"\\n===== {model_name} =====\")\n",
        "    print(\"F1 Score:\", f1)\n",
        "\n",
        "    # AUC Score\n",
        "    auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    print(\"AUC Score:\", auc)\n",
        "\n",
        "    # Classification Report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Evaluate all models with display\n",
        "evaluate_model(model_1, X_test, y_test, \"Model 1\")\n",
        "evaluate_model(model_2, X_test, y_test, \"Model 2\")\n",
        "evaluate_model(model_3, X_test, y_test, \"Model 3\")\n",
        "evaluate_model(model_4, X_test, y_test, \"Model 4\")\n"
      ],
      "metadata": {
        "id": "_7KyFN5Js59z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zgpljx0f8tuo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NtuE9LayDKXk",
        "LnTflF9nDY75",
        "HtJRaDg_DwBS",
        "NtLeCx_43IOZ",
        "3Ye8v9Yq3kQ7",
        "hRVzERSF3vRr"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}