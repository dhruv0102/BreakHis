{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMrpZ35YrEpJr8ssyRN3mCN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ejo_sC124Qr",
        "outputId": "2e9741e9-e375-48af-8745-16e910bd2eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up environment and mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "# This cell imports all necessary libraries and mounts Google Drive.\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Setting up environment and mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We load the 4 training CSVs, combine them, and immediately delete the\n",
        "# original DataFrames to free up RAM.\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/Wafer/'\n",
        "\n",
        "print(\"\\nLoading data...\")\n",
        "\n",
        "try:\n",
        "    if not os.path.exists(DATA_PATH):\n",
        "        raise FileNotFoundError\n",
        "\n",
        "    # The first 2 files have a header, which we use to define the columns\n",
        "    training_1 = pd.read_csv(f\"{DATA_PATH}training_1.csv\", delimiter=';', quotechar='\"')\n",
        "    training_4 = pd.read_csv(f\"{DATA_PATH}training_4.csv\", delimiter=';', quotechar='\"')\n",
        "    column_names = training_1.columns.tolist()\n",
        "\n",
        "    # The other files do not have a header\n",
        "    training_2 = pd.read_csv(f\"{DATA_PATH}training_2.csv\", sep=';', header=None, names=column_names, skiprows=1)\n",
        "    training_3 = pd.read_csv(f\"{DATA_PATH}training_3.csv\", sep=';', header=None, names=column_names, skiprows=1)\n",
        "\n",
        "    print(\"Combining datasets...\")\n",
        "    combined_data = pd.concat([training_1, training_2, training_3, training_4], ignore_index=True)\n",
        "\n",
        "    # --- Memory Management ---\n",
        "    print(\"Freeing up memory by deleting original dataframes...\")\n",
        "    del training_1, training_2, training_3, training_4\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Combined data loaded with shape: {combined_data.shape}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"FATAL ERROR: Could not find data files at '{DATA_PATH}'.\")\n",
        "    print(\"Please make sure the path is correct and that the files are present.\")\n",
        "    combined_data = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8mOGNp-3EE9",
        "outputId": "21fa10c2-0556-42fd-bbc1-9e1789efed80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading data...\n",
            "Combining datasets...\n",
            "Freeing up memory by deleting original dataframes...\n",
            "Combined data loaded with shape: (14007200, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4tY6bPbPhE6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}